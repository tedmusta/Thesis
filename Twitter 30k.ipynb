{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "respiratory-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk \n",
    "import string\n",
    "import re\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "inner-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Gebruiker/Desktop/Thesis/tweetslast30.csv', sep = ';')\n",
    "df.columns = ['Created-At', 'From-User-Id', 'To-User-Id', 'Language', 'Retweet-Count',\n",
    "       'PartyName', 'Id', 'Score', 'Scoring String', 'Negativity',\n",
    "       'Positivity', 'Uncovered Tokens', 'Total Tokens', 'tweet_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-emerald",
   "metadata": {},
   "source": [
    "## Data pre-processing \n",
    "\n",
    "Dataset wenmt through a pre-processing stage. First, only tweets which were written in english were left in the dataset. Out of the 30k rows dataset, about 3000 rows were dropped. Then, a filter for tweets which had no text body was applied, and therefore the dataset size was reduced to about 5000 rows. This is just a preliminary sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "unexpected-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering language english\n",
    "df = df[df['Language'] == 'en']\n",
    "df.dropna(subset = ['tweet_text'], inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-boston",
   "metadata": {},
   "source": [
    "#### NLP \n",
    "* removing puncuation\n",
    "* tokenized tweets\n",
    "* stopwords removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "headed-hardwood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.iloc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "given-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('http*.','', text)\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word.lower()) for word in text]\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "incident-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4873 Number of tweets has 14707 words\n"
     ]
    }
   ],
   "source": [
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(df['tweet_text'])\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "substantial-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_cleaned'] = df['tweet_text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "healthy-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in df['tweet_cleaned']:\n",
    "    for e in i:\n",
    "        corpus.append(e)\n",
    "        \n",
    "vocabulary = list(set(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "israeli-miniature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaargh</th>\n",
       "      <th>aajtak</th>\n",
       "      <th>aapolicyforum</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaronjmate</th>\n",
       "      <th>aartitikoo</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>맞넨</th>\n",
       "      <th>상어</th>\n",
       "      <th>상어상어</th>\n",
       "      <th>아</th>\n",
       "      <th>ﾉﾞ</th>\n",
       "      <th>𝗮𝗱𝗱𝗶𝗰𝘁</th>\n",
       "      <th>𝗱𝗼𝗱𝗴𝗶𝗻𝗴</th>\n",
       "      <th>𝗱𝗿𝗮𝗳𝘁</th>\n",
       "      <th>𝗱𝗿𝘂𝗴</th>\n",
       "      <th>𝗳𝗮𝘀𝗰𝗶𝘀𝘁</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 14707 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      a  aa  aaaargh  aajtak  aapolicyforum  aaron  aaronjmate  aartitikoo  \\\n",
       "0  0  0   0        0       0              0      0           0           0   \n",
       "1  1  0   0        0       0              0      0           0           0   \n",
       "2  1  1   0        0       0              0      0           0           0   \n",
       "3  1  0   0        0       0              0      0           0           0   \n",
       "4  1  1   0        0       0              0      0           0           0   \n",
       "\n",
       "   ab  ...  맞넨  상어  상어상어  아  ﾉﾞ  𝗮𝗱𝗱𝗶𝗰𝘁  𝗱𝗼𝗱𝗴𝗶𝗻𝗴  𝗱𝗿𝗮𝗳𝘁  𝗱𝗿𝘂𝗴  𝗳𝗮𝘀𝗰𝗶𝘀𝘁  \n",
       "0   0  ...   0   0     0  0   0       0        0      0     0        0  \n",
       "1   0  ...   0   0     0  0   0       0        0      0     0        0  \n",
       "2   0  ...   0   0     0  0   0       0        0      0     0        0  \n",
       "3   0  ...   0   0     0  0   0       0        0      0     0        0  \n",
       "4   0  ...   0   0     0  0   0       0        0      0     0        0  \n",
       "\n",
       "[5 rows x 14707 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-attention",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "* TF-DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "impaired-crawford",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.1 GiB for an array with shape (91841, 14707) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-366-49bdec8631c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n\u001b[0;32m      7\u001b[0m                  ('tfid', TfidfTransformer())]).fit(corpus)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.1 GiB for an array with shape (91841, 14707) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                 ('tfid', TfidfTransformer())]).fit(corpus)\n",
    "pipe['count'].transform(corpus).toarray()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe['tfid'].idf_\n",
    "\n",
    "\n",
    "a=pipe.fit_transform(corpus)\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "central-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1000)\n",
    "pca.fit(count_vect_df)\n",
    "\n",
    "X_pca = pca.transform(count_vect_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "chubby-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "smaller-decrease",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-377-8bda01b9c3d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m plt.scatter(pca.singular_values_[:, 0], pca.singular_values_[:, 1],\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdigits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'none'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             cmap=plt.cm.get_cmap('spectral', 10))\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'component 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'component 2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
